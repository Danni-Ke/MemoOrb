from pathlib import Path
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import gradio as gr
from dotenv import load_dotenv

from sentence_transformers import SentenceTransformer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, Document
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import BaseRetriever, Document
from langchain.prompts import PromptTemplate
from typing import List, Tuple
from sentence_transformers import CrossEncoder

# =========================
# ç¯å¢ƒä¸å¸¸é‡
# =========================
load_dotenv()
DB_DIR = "./chroma_db"
BOOKS_DIR = "./books"

# =========================
# åµŒå…¥æ¨¡å‹
# =========================
model_name = "BAAI/bge-m3"
test_model = SentenceTransformer(model_name)
sample_vec = test_model.encode("æµ‹è¯•å‘é‡")
print(f"âœ… å½“å‰ä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼š{model_name}ï¼Œè¾“å‡ºç»´åº¦ï¼š{len(sample_vec)}")
RERANKER = CrossEncoder("BAAI/bge-reranker-base", device="cpu")
EMBED_MODEL = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# =========================
# LLM
# =========================
LLM = ChatOpenAI(
    model_name="deepseek-chat",
    temperature=0,
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    openai_api_base=os.getenv("OPENAI_API_BASE")
)

# =========================
# VectorStore
# =========================
vectordb = Chroma(persist_directory=DB_DIR, embedding_function=EMBED_MODEL)

# ---------- å·¥å…·å‡½æ•° ----------
def _is_under(base_dir: str, p: Path) -> bool:
    """ä»…å…è®¸ books/ ç›®å½•å†…çš„è·¯å¾„"""
    try:
        p_resolved = p.resolve()
        base_resolved = Path(base_dir).resolve()
        p_resolved.relative_to(base_resolved)
        return True
    except Exception:
        return False

def _pick_source_from_metadata(meta: dict) -> Tuple[Path, str, int]:
    """
    ä» metadata é‡Œæå–æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€æ˜¾ç¤ºåä¸é¡µç ï¼›
    ä»…å…è®¸ books/ ä¸‹çœŸå®å­˜åœ¨çš„æ–‡ä»¶ã€‚
    """
    src = meta.get("book") or meta.get("source") or meta.get("file_path") or meta.get("title") or ""
    page = meta.get("page")
    p = Path(str(src))
    if not p.is_absolute():
        p = Path(BOOKS_DIR) / p.name
    if not p.exists() or not _is_under(BOOKS_DIR, p):
        raise FileNotFoundError("source not in books/")
    name = p.stem.strip()
    # page åˆæ³•åŒ–
    if isinstance(page, str) and page.isdigit():
        page = int(page)
    if not isinstance(page, int):
        page = None
    return p, name, page


def rerank(query, docs, top_k=5):
    if not docs:
        return []
    pairs = [(query, d.page_content[:3000]) for d in docs]  # é˜²æ­¢è¿‡é•¿
    scores = RERANKER.predict(pairs)
    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [d for d, _ in ranked[:top_k]]

# è‡ªå®šä¹‰ä¸€ä¸ªå¸¦ rerank çš„ retriever
class RerankRetriever:
    def __init__(self, vectordb, fetch_k=40, top_k=5):
        self.vectordb = vectordb
        self.fetch_k = fetch_k
        self.top_k = top_k

    def get_relevant_documents(self, query):
        docs = self.vectordb.similarity_search(query, k=self.fetch_k)
        return rerank(query, docs, self.top_k)
        
# =========================
#  QA æç¤ºè¯ï¼ˆä»¥æ–‡æ¡£ä¸ºä¸»ï¼Œè¯æ®ä¸è¶³å†æ‹’ç­”ï¼‰
# =========================
QA_PROMPT = PromptTemplate(
    input_variables=["context", "question", "chat_history"],
    template=(
        "ä½ æ˜¯å›¾ä¹¦é—®ç­”åŠ©æ‰‹ï¼Œå›ç­”æ—¶**ä¸»è¦ä¾æ®**ä¸‹é¢çš„<æ–‡æ¡£>å†…å®¹ï¼›"
        "è‹¥æ–‡æ¡£æ²¡æœ‰ç›´æ¥ç»™å‡ºç»“è®ºï¼Œå¯ä»¥åœ¨ä¸å¼•å…¥å¤–éƒ¨çŸ¥è¯†çš„å‰æä¸‹ï¼Œ"
        "å¯¹æ–‡æ¡£é‡Œçš„ç›¸å…³ä¿¡æ¯è¿›è¡Œ**ä¸¥è°¨çš„æ¦‚æ‹¬æˆ–å½’çº³**ï¼›"
        "å¦‚æœæ–‡æ¡£æœªåŒ…å«ä¸é—®é¢˜é«˜åº¦ç›¸å…³çš„å†…å®¹ï¼ˆå¦‚ä¹¦åã€å…³é”®æ¦‚å¿µï¼‰ï¼Œ"
        "å¿…é¡»å›ç­”ï¼šâ€œå½“å‰çŸ¥è¯†åº“æœªåŒ…å«ç›¸å…³ä¿¡æ¯ã€‚è¯·æ·»åŠ ç›¸å…³èµ„æ–™åå†æé—®ï¼â€\n\n"
        "<å†å²å¯¹è¯>\n{chat_history}\n\n"
        "<æ–‡æ¡£>\n{context}\n\n"
        "<é—®é¢˜>\n{question}\n\n"
        "è¯·ç»™å‡ºç®€æ´ã€ä¾æ®å……åˆ†çš„ä¸­æ–‡å›ç­”ã€‚"
    )
)


# =========================
# è‡ªå®šä¹‰â€œé˜ˆå€¼æ£€ç´¢å™¨â€
# =========================
class ThresholdRerankRetriever(BaseRetriever):
    """å…ˆå‘é‡å¬å›/é˜ˆå€¼è¿‡æ»¤ï¼Œå†ç”¨äº¤å‰ç¼–ç å™¨é‡æ’ã€‚"""
    vectorstore: Chroma
    k: int = 4                   # æœ€ç»ˆè¿”å›æ¡æ•°
    fetch_k: int = 40            # å€™é€‰æ± ï¼ˆç”¨äºé‡æ’ï¼‰
    score_threshold: float = 0.20
    min_keep: int = 3            # é˜ˆå€¼è¿‡ä¸¥æ—¶è‡³å°‘ä¿ç•™ N æ¡
    reranker: CrossEncoder = None
    rerank_top_k: int = 4        # é‡æ’åå–å‰ N æ¡ï¼ˆä¸€èˆ¬ç­‰äº kï¼‰

    # å…¼å®¹ä¸åŒåˆ†æ•°è¯­ä¹‰ï¼šè¿”å› 0~1 ç›¸ä¼¼åº¦ï¼Œè¶Šå¤§è¶Šç›¸å…³
    def _normalize_pairs(self, pairs):
        norm = []
        for d, s in pairs:
            if s is None:
                score = 0.0
            elif 0 <= s <= 1:
                score = float(s)       # å·²æ˜¯ç›¸ä¼¼åº¦
            else:
                score = 1.0 / (1.0 + float(s))  # è·ç¦»â†’ç›¸ä¼¼åº¦
            norm.append((d, score))
        return norm

    def _get_relevant_documents(self, query: str) -> List[Document]:
        # 1) ç²—å¬å›ï¼ˆå–å¤§ä¸€äº›ç»™é‡æ’ç”¨ï¼‰
        try:
            pairs = self.vectorstore.similarity_search_with_relevance_scores(query, k=self.fetch_k)
            pairs = self._normalize_pairs(pairs)
        except Exception:
            # å…œåº•ï¼šæ²¡æœ‰åˆ†æ•°æ¥å£å°±ç›´æ¥å–
            docs = self.vectorstore.similarity_search(query, k=self.fetch_k)
            pairs = [(d, 1.0) for d in docs]

        # 2) é˜ˆå€¼è¿‡æ»¤ + è‡³å°‘ä¿åº•
        filtered = [(d, sc) for d, sc in pairs if sc >= self.score_threshold]
        filtered.sort(key=lambda x: x[1], reverse=True)
        if len(filtered) < self.min_keep and pairs:
            pairs.sort(key=lambda x: x[1], reverse=True)
            need = self.min_keep - len(filtered)
            for d, sc in pairs:
                if (d, sc) not in filtered:
                    filtered.append((d, sc))
                    if len(filtered) >= self.min_keep:
                        break
        candidates = [d for d, _ in filtered] or [d for d, _ in pairs]

        if not candidates:
            # åŒå…œåº•ï¼šMMR â†’ æ™®é€šç›¸ä¼¼
            try:
                return self.vectorstore.max_marginal_relevance_search(query, k=self.k, fetch_k=self.fetch_k)
            except Exception:
                return self.vectorstore.similarity_search(query, k=self.k)

        # 3) é‡æ’ï¼ˆCross-Encoderï¼‰
        try:
            if self.reranker is not None:
                pairs_ce = [(query, d.page_content[:3000]) for d in candidates]  # æˆªæ–­é˜²è¶…é•¿
                scores = self.reranker.predict(pairs_ce)  # numpy array
                ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
                return [d for d, _ in ranked[: self.rerank_top_k]]
        except Exception:
            # é‡æ’å¤±è´¥å°±å¿½ç•¥é‡æ’
            pass

        # 4) ä¸é‡æ’/é‡æ’å¤±è´¥æ—¶æŒ‰ç›¸ä¼¼åº¦å–å‰ k
        return candidates[: self.k]

# âœ… å®ä¾‹åŒ–è‡ªå®šä¹‰æ£€ç´¢å™¨ï¼ˆâ† ä½ é—®çš„â€œretriever åœ¨å“ªå„¿å®ä¾‹åŒ–â€å°±åœ¨è¿™é‡Œï¼‰
retriever = ThresholdRerankRetriever(
    vectorstore=vectordb,
    k=4,
    fetch_k=40,            # ç»™é‡æ’æ›´å¤§çš„å€™é€‰æ± 
    score_threshold=0.28,  # 0.15 æ›´æ”¾å¼€ / 0.30 æ›´ä¿å®ˆ
    min_keep=3,
    reranker=RERANKER,
    rerank_top_k=4
)

# =========================
# Memory & QA Chain
# =========================
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    input_key="question",
    output_key="answer"
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=LLM,
    retriever=retriever,
    memory=memory,
    return_source_documents=True,
    combine_docs_chain_kwargs={"prompt": QA_PROMPT}
)

# =========================
# å¼•ç”¨æ ¼å¼åŒ–ï¼ˆä»… books/ ä¸­ä¸”æœ¬æ¬¡çœŸçš„è¢«å‘½ä¸­çš„æ–‡æ¡£ï¼‰
# =========================
def format_refs(docs: List[Document], max_refs: int = 3) -> str:
    refs = []
    seen = set()
    for d in docs:
        meta = (d.metadata or {})
        try:
            p, name, page = _pick_source_from_metadata(meta)
        except Exception:
            # é books/ æˆ–æ— æ•ˆæºï¼Œè·³è¿‡
            continue
        ref = f"ã€Š{name}ã€‹" + (f" - ç¬¬{int(page)}é¡µ" if page else "")
        if ref not in seen:
            seen.add(ref)
            refs.append(ref)
        if len(refs) >= max_refs:  # é™åˆ¶æœ€å¤š N æ¡
            break
    if not refs:
        return "å‚è€ƒæ¥æºï¼šæ— "
    return "å‚è€ƒæ¥æºï¼š" + "ï¼›".join(refs)

# =========================
# â€œä½ è„‘å­é‡Œæœ‰ä»€ä¹ˆâ€ç±»é—®é¢˜åˆ¤æ–­ & å±•ç¤º
# =========================
def is_meta_question(q: str) -> bool:
    ql = q.lower()
    keywords = [
        "ä½ è„‘å­é‡Œ", "ä½ éƒ½çŸ¥é“", "ä½ è®°å¾—ä»€ä¹ˆ", "ä½ è®°å¾—å“ªäº›",
        "ä½ æœ‰å“ªäº›å†…å®¹", "ä½ æœ‰ä»€ä¹ˆèµ„æ–™", "ä½ èƒ½å›ç­”ä»€ä¹ˆ",
        "ä½ èƒ½å›ç­”å“ªäº›", "ä½ çš„çŸ¥è¯†", "ä½ ä¼šä»€ä¹ˆ", "ä½ æ‡‚ä»€ä¹ˆ",
        "what do you know", "what do you remember",
        "what's in your brain", "what can you answer",
        "what info do you have", "your knowledge",
        "what topics do you know", "what books do you remember",
        "do you have any info", "what sources do you have",
    ]
    return any(k in q or k in ql for k in keywords)

def describe_knowledge_base():
    try:
        coll = getattr(vectordb, "_collection", None)
        if coll is None:
            return "âš ï¸ å‘é‡åº“æœªåŠ è½½ï¼Œè¯·å…ˆæ„å»ºçŸ¥è¯†åº“ã€‚"
        metas = (coll.get(include=["metadatas"]) or {}).get("metadatas") or []
        sources = set()
        for meta in metas:
            if not meta:
                continue
            # ä»…ç»Ÿè®¡ books/ ä¸‹å¯ç”¨æ–‡ä»¶
            try:
                p, name, _ = _pick_source_from_metadata(meta)
                if p.exists():
                    sources.add(name)
            except Exception:
                continue
        if not sources:
            return "ğŸ˜¢ å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆå¯¼å…¥å¹¶æ„å»ºçŸ¥è¯†åº“ã€‚"
        return "\n\n" + "\n".join(f"â€¢ ã€Š{s}ã€‹" for s in sorted(sources))
    except Exception as e:
        return f"è¯»å–å‘é‡åº“å¤±è´¥ï¼š{e}"

def list_books_from_folder(folder_path=BOOKS_DIR):
    supported_exts = [".pdf", ".epub", ".txt", ".md"]
    if not os.path.exists(folder_path):
        return "ğŸ“‚ æœªæ‰¾åˆ° books æ–‡ä»¶å¤¹ã€‚"
    files = [
        Path(f).stem.strip().replace("_", " ")
        for f in os.listdir(folder_path)
        if any(f.lower().endswith(ext) for ext in supported_exts)
    ]
    if not files:
        return "ğŸ“‚ books æ–‡ä»¶å¤¹ä¸­æš‚æ— æ”¯æŒçš„æ–‡ä»¶ã€‚"
    return "\n\n" + "\n".join(f"â€¢ ã€Š{f}ã€‹" for f in sorted(files))

# =========================
# ä¸»ç­”å¤å‡½æ•°ï¼ˆä¸¥æ ¼æ‹’ç­” & ä»…è¿½åŠ å®‰å…¨å¼•ç”¨ï¼‰
# =========================
def answer_question(query, history):
    if is_meta_question(query):
        return describe_knowledge_base()

    result = qa_chain({"question": query})
    answer = (result.get("answer") or "").strip()
    source_docs = result.get("source_documents") or []

    # âœ… å¦‚æœæ²¡æœ‰å‘½ä¸­ä»»ä½•æ–‡æ¡£ï¼Œå¼ºåˆ¶æ‹’ç­”
    if not source_docs:
        return "å½“å‰çŸ¥è¯†åº“æœªåŒ…å«ç›¸å…³ä¿¡æ¯ã€‚è¯·æ·»åŠ ç›¸å…³èµ„æ–™åå†æé—®ï¼"

    # ä¿å­˜ç®€æ˜å¯¹è¯è®°å¿†
    qa_chain.memory.save_context({"question": query}, {"answer": answer})

    # âœ… ç”µå­æ …æ ï¼šå¦‚æœæ¨¡å‹è°ƒçš®ï¼ˆæ¯”å¦‚è¯´â€œè™½ç„¶æ–‡æ¡£æ²¡æï¼Œä½†æˆ‘æ¨æµ‹â€¦â€ï¼‰ï¼Œä»ç„¶æ‹¦æ‰
    reject_triggers = [
        "æœªåŒ…å«ç›¸å…³ä¿¡æ¯", "æ— æ³•ä½œç­”", "æ²¡æœ‰æ‰¾åˆ°", "èµ„æ–™ä¸è¶³",
        "not enough information", "unable to answer", "out of scope"
    ]
    if any(p in answer for p in reject_triggers):
        return "å½“å‰çŸ¥è¯†åº“æœªåŒ…å«ç›¸å…³ä¿¡æ¯ã€‚è¯·æ·»åŠ ç›¸å…³èµ„æ–™åå†æé—®ï¼"

    # âœ… æ­£å¸¸å›ç­”æ‰é™„å¸¦å¼•ç”¨
    refs = format_refs(source_docs, max_refs=3)
    return f"{answer}\n\n{refs}"




# =========================
# è¿è¡Œ ingest
# =========================
def run_ingest():
    try:
        import ingest
        ingest.main()
        return "âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼"
    except Exception as e:
        return f"âŒ æ„å»ºå¤±è´¥ï¼š{e}"

# =========================
# Gradio UIï¼ˆäº‹ä»¶å‡½æ•°åœ¨å‰ï¼‰
# =========================

# ä¿å­˜èŠå¤©å†å²ï¼ˆ[(user, assistant), ...]ï¼‰
# æ³¨æ„ï¼šçœŸæ­£çš„ gr.State ä¼šåœ¨ Blocks é‡Œåˆ›å»ºï¼›è¿™é‡Œåªæ˜¯å‡½æ•°ç­¾åç”¨
def on_send(user_msg, history):
    user_msg = (user_msg or "").strip()
    if not user_msg:
        return gr.update(), history, history
    answer = answer_question(user_msg, history)
    history = history + [(user_msg, answer)]
    return "", history, history

def on_retry(history):
    if not history:
        return history, history
    last_user = history[-1][0]
    new_answer = answer_question(last_user, history[:-1])
    history = history[:-1] + [(last_user, new_answer)]
    return history, history

def on_undo(history):
    if not history:
        return history, history
    history = history[:-1]
    return history, history

def on_clear():
    return [], []

def update_lang(lang):
    if lang == "ä¸­æ–‡":
        return (
            gr.update(label="ğŸ“˜ ä½ çš„å›¾ä¹¦é¦†"),
            gr.update(label="ğŸ“š ä½ çš„çŸ¥è¯†åº“"),
            gr.update(label="ğŸ› ï¸ çŠ¶æ€æç¤º"),
            gr.update(value="ğŸ” é‡æ–°æ„å»ºçŸ¥è¯†åº“"),
            "### ğŸ“šå£è¢‹è®°å¿† Â· MemoOrb",
            "ğŸŒ¿ å°†ä½ è¦é˜…è¯»çš„ä¹¦ç±ï¼ˆPDF / EPUB / TXT / MDï¼‰æ”¾å…¥ `books/` æ–‡ä»¶å¤¹ï¼Œç‚¹å‡»å·¦ä¾§æŒ‰é’®åˆ·æ–°çŸ¥è¯†åº“ï¼Œç„¶åå°±å¯ä»¥ç›´æ¥å’Œä½ çš„ä¹¦æœ¬å¯¹è¯ã€‚ğŸŒ¿",
            gr.update(label="ğŸ’¬ å¯¹è¯ / Chat"),
            gr.update(label="âœï¸ æ¶ˆæ¯ / Message", placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜â€¦"),
            gr.update(value="â–¶ï¸ å‘é€ / Send"),
            gr.update(value="ğŸ”„ é‡è¯• / Retry"),
            gr.update(value="â†©ï¸ æ’¤å› / Undo"),
            gr.update(value="ğŸ—‘ï¸ æ¸…ç©º / Clear"),
        )
    else:
        return (
            gr.update(label="ğŸ“˜ Books Library"),
            gr.update(label="ğŸ“š Knowledge Base"),
            gr.update(label="ğŸ› ï¸ Status"),
            gr.update(value="ğŸ” Rebuild Knowledge Base"),
            "### ğŸ“š MemoOrb Â· Pocket Memory",
            "ğŸŒ¿ Put your books (PDF / EPUB / TXT / MD) into `books/`, click the button on the left to rebuild, then chat with your books. ğŸŒ¿",
            gr.update(label="ğŸ’¬ Chat"),
            gr.update(label="âœï¸ Message", placeholder="Type your questionâ€¦"),
            gr.update(value="â–¶ï¸ Send"),
            gr.update(value="ğŸ”„ Retry"),
            gr.update(value="â†©ï¸ Undo"),
            gr.update(value="ğŸ—‘ï¸ Clear"),

        )

with gr.Blocks() as demo:
    with gr.Row():
    # ===== å·¦æ ï¼šæŠŠè¯­è¨€åˆ‡æ¢æ”¾åœ¨æœ€ä¸Šé¢ =====
        with gr.Column(scale=1):
            lang_selector = gr.Dropdown(
                choices=["ä¸­æ–‡", "English"],
                value="ä¸­æ–‡",
                label="ğŸŒ Language / è¯­è¨€",
                scale=1,            # è·Ÿ Column åŒæ­¥
                container=True      # ä½¿ç”¨ä¸å…¶ä»–æ§ä»¶ä¸€è‡´çš„å®¹å™¨ï¼Œè§†è§‰ç»Ÿä¸€
            )

            books_box  = gr.Textbox(value=list_books_from_folder(), lines=8, label="ğŸ“˜ ä½ çš„å›¾ä¹¦é¦†")
            kb_box     = gr.Textbox(value=describe_knowledge_base(), lines=8, label="ğŸ“š ä½ çš„çŸ¥è¯†åº“")
            status_box = gr.Textbox(label="ğŸ› ï¸ çŠ¶æ€æç¤º", interactive=False)
            update_btn = gr.Button("ğŸ” é‡æ–°æ„å»ºçŸ¥è¯†åº“")

            def run_ingest_and_refresh():
                msg = run_ingest()
                return list_books_from_folder(), describe_knowledge_base(), msg
            update_btn.click(run_ingest_and_refresh, outputs=[books_box, kb_box, status_box])

        # ===== å³æ ï¼šæ ‡é¢˜/æè¿° + èŠå¤©åŒº =====
        with gr.Column(scale=3):
            title_md = gr.Markdown("### ğŸ“šå£è¢‹è®°å¿† Â· MemoOrb")
            desc_md  = gr.Markdown("ğŸŒ¿ å°†ä½ è¦é˜…è¯»çš„ä¹¦ç±ï¼ˆPDF / EPUB / TXT / MDï¼‰æ”¾å…¥ `books/` æ–‡ä»¶å¤¹ï¼Œç‚¹å‡»å·¦ä¾§æŒ‰é’®åˆ·æ–°çŸ¥è¯†åº“ï¼Œç„¶åå°±å¯ä»¥ç›´æ¥å’Œä½ çš„ä¹¦æœ¬å¯¹è¯ã€‚ğŸŒ¿")

            chat = gr.Chatbot(label="ğŸ’¬ å¯¹è¯ / Chat", height=480)
            # ---- é¡¶éƒ¨å³ä¾§ï¼šå‘é€ï¼ˆå•ç‹¬ä¸€è¡Œé å³ï¼‰----
            with gr.Row():
                msg = gr.Textbox(
                    label="âœï¸ æ¶ˆæ¯ / Message",
                    placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜â€¦",
                    lines=1,
                    scale=9
                )
                send_btn = gr.Button("â–¶ï¸ å‘é€ / Send", variant="primary", scale=1)
            # â€”â€” åœ¨è¾“å…¥æ¡†ä¸‹é¢æ”¾ä¾‹å­
            examples = gr.Examples(
                examples=[
                    ["ã€Šä¹Œåˆä¹‹ä¼—ã€‹çš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"],
                    ["ä½ è„‘å­é‡Œæœ‰ä»€ä¹ˆä¹¦æœ¬å•Šï¼Ÿ"],
                    ["å­˜åœ¨ä¸»ä¹‰å’Œè™šæ— ä¸»ä¹‰çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"]
                ],
                inputs=msg
            )

            # ---- ä¸‹ä¸€è¡Œï¼šé‡è¯• / æ’¤å› / æ¸…ç©º ----
            with gr.Row():
                retry_btn = gr.Button("ğŸ”„ é‡è¯• / Retry")
                undo_btn  = gr.Button("â†©ï¸ æ’¤å› / Undo")
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º / Clear")

            # ä¼šè¯çŠ¶æ€ï¼ˆå¿…é¡»åœ¨ Blocks å†…ï¼‰
            chat_state = gr.State([])

            # äº‹ä»¶ç»‘å®šï¼ˆå¿…é¡»åœ¨ Blocks å†…ï¼Œä¸”å‡½æ•°å·²åœ¨ä¸Šæ–¹å®šä¹‰ï¼‰
            send_btn.click(on_send, [msg, chat_state], [msg, chat_state, chat])
            msg.submit(on_send, [msg, chat_state], [msg, chat_state, chat])
            retry_btn.click(on_retry, [chat_state], [chat_state, chat])
            undo_btn.click(on_undo, [chat_state], [chat_state, chat])
            clear_btn.click(on_clear, None, [chat_state, chat])

    # ===== è¯­è¨€åˆ‡æ¢è”åŠ¨ï¼ˆé¡¶æ é€‰æ‹©å™¨ç”Ÿæ•ˆï¼‰=====
    lang_selector.change(
        update_lang,
        inputs=[lang_selector],
        outputs=[
            books_box, kb_box, status_box, update_btn,  # å·¦ä¾§
            title_md, desc_md,                          # å³ä¾§æ ‡é¢˜/æè¿°
            chat, msg,                                  # èŠå¤©åŒºæ ‡ç­¾/å ä½
            send_btn, retry_btn, undo_btn, clear_btn,    # å››ä¸ªæŒ‰é’®æ–‡æ¡ˆ
        ]
    )

if __name__ == "__main__":
    demo.launch()
